# ü§ó Loading Models and Inference with Hugging Face

> **Laborat√≥rio pr√°tico** do curso IBM AI Engineering Professional Certificate

[![IBM Skills Network](https://img.shields.io/badge/IBM-Skills_Network-052FAD?style=flat&logo=ibm)](https://www.ibm.com/training/)
[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.3.1-EE4C2C?style=flat&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ü§ó_Transformers-4.40.0-FFD21E?style=flat)](https://huggingface.co/transformers/)

---

## üìö Sobre este Lab

Este reposit√≥rio cont√©m exerc√≠cios pr√°ticos do **IBM AI Engineering Professional Certificate**, especificamente do m√≥dulo sobre carregamento de modelos e infer√™ncia usando a biblioteca Hugging Face Transformers.

**‚ö†Ô∏è Importante:** Este √© um laborat√≥rio guiado para fins educacionais, n√£o um projeto original. O objetivo √© aprender e praticar conceitos fundamentais de NLP com modelos pr√©-treinados.

---

## üéØ Objetivos de Aprendizado

Neste laborat√≥rio, foram explorados os seguintes conceitos:

### 1Ô∏è‚É£ **Infer√™ncia Manual (Baixo N√≠vel)**
- Carregamento manual de modelos e tokenizers
- Pr√©-processamento de texto (tokeniza√ß√£o)
- Execu√ß√£o de infer√™ncia com `torch.no_grad()`
- Processamento de logits e sa√≠das do modelo
- Decodifica√ß√£o de resultados

### 2Ô∏è‚É£ **Pipeline API (Alto N√≠vel)**
- Uso da fun√ß√£o `pipeline()` do Hugging Face
- Simplifica√ß√£o de tarefas de NLP com poucas linhas de c√≥digo
- Compara√ß√£o entre abordagens manual vs. automatizada

---

## üß™ Tarefas Implementadas

### **Text Classification (Sentiment Analysis)**
- **Modelo:** `distilbert-base-uncased-finetuned-sst-2-english`
- **Tarefa:** Classifica√ß√£o de sentimentos (POSITIVE/NEGATIVE)
- **Implementa√ß√£o:** Manual + Pipeline

### **Text Generation**
- **Modelo:** `gpt2`
- **Tarefa:** Gera√ß√£o de texto a partir de um prompt
- **Implementa√ß√£o:** Manual + Pipeline

### **Language Detection**
- **Modelo:** `papluca/xlm-roberta-base-language-detection`
- **Tarefa:** Detec√ß√£o de idioma de textos
- **Implementa√ß√£o:** Pipeline

### **Translation (Text-to-Text)**
- **Modelo:** `t5-small`
- **Tarefa:** Tradu√ß√£o de ingl√™s para franc√™s
- **Implementa√ß√£o:** Pipeline

### **Fill-Mask**
- **Modelo:** `bert-base-uncased`
- **Tarefa:** Preenchimento de tokens mascarados
- **Implementa√ß√£o:** Pipeline

---

## üõ†Ô∏è Tecnologias Utilizadas

| Tecnologia | Vers√£o | Prop√≥sito |
|------------|--------|-----------|
| **Python** | 3.12 | Linguagem de programa√ß√£o |
| **PyTorch** | 2.3.1 | Framework de deep learning |
| **Transformers** | 4.40.0 | Biblioteca de modelos pr√©-treinados |
| **Torchvision** | 0.18.0 | Utilit√°rios de vis√£o computacional |

---

## üì¶ Instala√ß√£o

### Requisitos
```bash
pip install torch~=2.3.1
pip install torchvision~=0.18.0
pip install transformers~=4.40.0
```

### Executar o Notebook
```bash
jupyter notebook Loading_Models_and_Inference_with_Hugging_Face.ipynb
```

---

## üìñ Estrutura do Notebook

```
1. Setup e Instala√ß√£o de Bibliotecas
2. Text Classification com DistilBERT
   ‚îú‚îÄ‚îÄ Carregamento manual do modelo
   ‚îú‚îÄ‚îÄ Pr√©-processamento do texto
   ‚îú‚îÄ‚îÄ Infer√™ncia e processamento de sa√≠das
   ‚îî‚îÄ‚îÄ Implementa√ß√£o com pipeline()
3. Text Generation com GPT-2
   ‚îú‚îÄ‚îÄ Carregamento manual do modelo
   ‚îú‚îÄ‚îÄ Gera√ß√£o de texto passo a passo
   ‚îî‚îÄ‚îÄ Implementa√ß√£o com pipeline()
4. Hugging Face Pipeline API
   ‚îú‚îÄ‚îÄ Text Classification
   ‚îú‚îÄ‚îÄ Language Detection
   ‚îú‚îÄ‚îÄ Text Generation (T5)
   ‚îî‚îÄ‚îÄ Fill-Mask (BERT)
5. Exerc√≠cio Pr√°tico: Fill-Mask
```

---

## üí° Principais Aprendizados

### **Abordagem Manual vs. Pipeline**

| Aspecto | Manual | Pipeline |
|---------|--------|----------|
| **C√≥digo** | ~15-20 linhas | 3-5 linhas |
| **Controle** | Total | Limitado |
| **Flexibilidade** | Alta | M√©dia |
| **Facilidade** | Requer conhecimento | Muito f√°cil |
| **Uso recomendado** | Produ√ß√£o customizada | Prototipagem r√°pida |

### **Quando usar cada abordagem?**

‚úÖ **Use `pipeline()` quando:**
- Prototipando rapidamente
- Tarefas comuns de NLP
- Simplicidade √© prioridade
- Deploy r√°pido

‚úÖ **Use abordagem manual quando:**
- Precisa de controle fino
- Otimiza√ß√£o de performance
- Tarefas customizadas
- Integra√ß√£o complexa

---

## üîç Exemplos de C√≥digo

### Text Classification (Sentimento)

**Abordagem Manual:**
```python
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

text = "I love this product!"
inputs = tokenizer(text, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    
logits = outputs.logits
predicted_class = torch.argmax(logits, dim=-1).item()
```

**Com Pipeline:**
```python
from transformers import pipeline

classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
result = classifier("I love this product!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]
```

---

## üìä Resultados e Observa√ß√µes

### **Performance dos Modelos**

- **DistilBERT:** Alta acur√°cia em classifica√ß√£o de sentimentos, r√°pido
- **GPT-2:** Gera√ß√£o coerente, por√©m com repeti√ß√µes ocasionais
- **T5:** Excelente para tarefas de tradu√ß√£o
- **BERT:** √ìtimo para fill-mask e MLM tasks

### **Compara√ß√£o de Efici√™ncia**

| Tarefa | Tempo Manual | Tempo Pipeline | Linhas de C√≥digo (Manual) | Linhas de C√≥digo (Pipeline) |
|--------|--------------|----------------|---------------------------|----------------------------|
| Classifica√ß√£o | ~2-3s | ~1-2s | 15-20 | 3-5 |
| Gera√ß√£o | ~3-5s | ~2-3s | 20-25 | 3-5 |
| Tradu√ß√£o | N/A | ~2-3s | N/A | 3-5 |

---

## üéì Certifica√ß√£o

Este laborat√≥rio faz parte do **IBM AI Engineering Professional Certificate** oferecido via IBM Skills Network / Coursera.

**T√≥picos do Curso:**
- Machine Learning com Python
- Deep Learning e Neural Networks
- Computer Vision
- Natural Language Processing
- Generative AI e LLMs

---

## üìù Notas Importantes

1. **Requisitos de Hardware:** Alguns modelos podem requerer GPU para execu√ß√£o otimizada
2. **Tempo de Download:** Primeira execu√ß√£o baixa os modelos (pode levar minutos)
3. **Warnings:** Warnings sobre weights n√£o utilizados s√£o normais para modelos fine-tuned
4. **Vers√µes:** Testado com Python 3.12 e bibliotecas especificadas

---

## üîó Recursos Adicionais

- [Documenta√ß√£o Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Hugging Face Model Hub](https://huggingface.co/models)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [IBM Skills Network](https://skills.network/)

---

## üìÑ Licen√ßa

Este material √© parte do conte√∫do educacional da IBM e √© usado apenas para fins de aprendizado.

---

## ‚úçÔ∏è Autor

Desenvolvido como parte dos estudos para a **IBM AI Engineering Professional Certificate**

**Contexto:** Laborat√≥rio guiado - Exerc√≠cio de aprendizado, n√£o projeto original

---

## üè∑Ô∏è Tags

`#IBM` `#AI` `#MachineLearning` `#NLP` `#HuggingFace` `#Transformers` `#PyTorch` `#DeepLearning` `#SentimentAnalysis` `#TextGeneration` `#Education` `#Certification`
